# Uni-Adapter
This repository is the official implementation of the AAAI 2026 paper ["Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models"](http://arxiv.org/link).

ðŸš§ **Code Release Notice**

The code and pretrained checkpoints are currently being prepared for public release.  
Please stay tuned â€” the full implementation will be made available **soon**.


## Overview

3D Vision-Language Foundation Models (VLFMs) have shown strong generalization and zero-shot recognition capabilities in open-world point cloud processing tasks. However, these models often underperform in practical scenarios where data are noisy, incomplete, or drawn from a different distribution than the training data. To address this, we propose **Uni-Adapter**, a novel training-free online test-time adaptation (TTA) strategy for 3D VLFMs based on dynamic prototype learning. We define a 3D cache to store class-specific cluster centers as prototypes, which are continuously updated to capture intra-class variability in heterogeneous data distributions. These dynamic prototypes serve as anchors for cache-based logit computation via similarity scoring. Simultaneously, a graph-based label smoothing module captures inter-prototype similarities to enforce label consistency among similar prototypes. Finally, we unify predictions from the original 3D VLFM and the refined 3D cache using entropy-weighted aggregation for reliable adaptation. Without retraining, Uni-Adapter effectively mitigates distribution shifts, achieving state-of-the-art performance on diverse 3D benchmarks over different 3D VLFMsâ€”improving ModelNet-40C by 10.55%, ScanObjectNN-C by 8.26%, and ShapeNet-C by 4.49% over the source 3D VLFMs.

## Motivation



## Environment


### Package Setup
* Ubuntu 23.10
* Python 3.8.16
* PyTorch 1.12.0
* CUDA 11.6
* torchvision 0.13.0
* timm 0.9.16
* pueue & pueued 2.0.4

```sh
# NOTE: Option 1 is recommended. A complete package list is available in `env.yaml`.

# Option 1: Manually create a Conda virtual environment
conda create -n uniadapter python=3.8.16
conda activate uniadapter

# Install PyTorch
pip install torch==1.12.0+cu116 torchvision==0.13.0+cu116 --extra-index-url https://download.pytorch.org/whl/cu116

# Install Dassl
git clone https://github.com/...
cd dassl/
python setup.py develop  # No need to rebuild if the source code is modified.

# Option 2: Create the Conda environment from the provided env.yaml
conda env create -f env.yaml

```


### Pre-trained Weights
1. We use the following models as baselines, and their pre-trained weights can be obtained from their public GitHub repositories.
    * [ULIP-2](https://huggingface.co/datasets/auniquesun/Point-PRC/tree/main/pretrained-weights/ulip-2)
    * [OpenShape](https://github.com/Colin97/OpenShape_code/)
    * [Uni3D](https://github.com/baaivision/Uni3D)

    - **NOTE:** 
        1. ULIP-2 uses the same [text encoder](https://huggingface.co/datasets/auniquesun/Point-PRC/tree/main/pretrained-weights/ulip/image-text-encoder) 
        2. For OpenShape, we use the [pointbert-vitg14-rgb](https://huggingface.co/OpenShape/openshape-pointbert-vitg14-rgb/tree/main) version
            - For text encoder in OpenShape, we use [CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k) from **huggingface laion**
        3. For Uni3D, we use the [uni3d-L](https://huggingface.co/BAAI/Uni3D/tree/main/modelzoo/uni3d-L) version
            - For text encoder in Uni3D, we use [eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k](https://huggingface.co/timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k) from **huggingface timm**


### Datasets
1. The folder structure of used datasets should be organized as follows.
```sh
    
  |----data   
      |----modelnet_c
      |----shapenet_c
      |----scanobjnn_c
      |----omniobject3d
          |----1024
      |----objaverse_lvis


    ...
```

### Evaluation on _ModelNet-C_




## Acknowledgement
Our implementation is partially inspired by the following projects, thanks to their great work.

1. [Point-Cache](https://github.com/auniquesun/Point-Cache)
2. [ULIP](https://github.com/salesforce/ULIP)
3. [OpenShape](https://github.com/ZrrSkywalker/PointCLIP)
4. [Uni3D](https://github.com/yangyangyang127/PointCLIP_V2)
5. [TDA](https://github.com/kdiAAA/TDA)

## Contact
If you have any question about our work, please search related issues or create a new one in this repository.
